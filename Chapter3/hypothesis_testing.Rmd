---
title: "Introduction to random sampling and hypothesis testing"
author: "Research Computing and Data Science: Jesus Urtasun; jurtasun@ic.ac.uk"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  rmdformats::robobook:
    self_contained: true
    code_folding: hide
    thumbnails: false
    lightbox: true
    gallery: false
    highlight: tango
    number_sections: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include = FALSE}

# knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, echo = T, message = FALSE, cache.lazy = FALSE)

```

```{r import, cache = T, echo = T, eval = T, warnings = F, messages = F}

# Import libraries

# Set working directory
workdir <- "/Users/jurtasun/Desktop/Courses/ICL/RCDS_Introduction_sampling_hypothesis_testing/Chapter3"
setwd(workdir)

```

# Chapter 3

## 3.1 Hypothesis testing

We will now discuss one of the main topics of statistical inference. Once we have described random events, discrete and continous distributions, and how to build confidence intervals, we are ready to start formulating hypothesis and quantify the accuracy of given predictions.

Wi will first discuss the null and alternative hypothesis, and then the idea of significance and surprise. Once a hypothesis on a particular process has been stated, one can take the observed results and infer how likely was a specific event to happen. The mathematical object we will use to quantify such surprise, meaning how likely / unlikely was to observe a specific even given the previous hypothesis, is referred to as p-value.

### Exampele 1: probability of rolling a six

```{r hypothesis_testing, cache = T, echo = T, eval = T, warnings = F, messages = F}

data <- c(6, 1, 5, 6, 2, 6, 4, 3, 4, 6, 1, 2, 5, 6, 6, 3, 6, 2, 6, 4, 6, 2,
       5, 4, 2, 3, 3, 6, 6, 1, 2, 5, 6, 4, 6, 2, 1, 3, 6, 5, 4, 5, 6, 3,
       6, 6, 1, 4, 6, 6, 6, 6, 6, 2, 3, 1, 6, 4, 3, 6, 2, 4, 6, 6, 6, 5,
       6, 2, 1, 6, 6, 4, 3, 6, 5, 6, 6, 2, 6, 3, 6, 6, 1, 4, 6, 4, 2, 6,
       6, 5, 2, 6, 6, 4, 3, 1, 6, 6, 5, 5)


# We will work with the binomial distribution for the observed number of sixes

# Write down the hypotheses
# H0: p = 1/6
# H1: p > 1/6

# choose a significance level
# alpha = 0.01


# code the data as 6=success and {0-5}=failure
six <- data==6
print(six)

# how many sixes were observed?
x <- sum(six)
x

# check number of trials
n <- length(data)
n

#  now use H0 to find the p-value of the observed number of sixes
pval <- 1 - pbinom(42,100,1/6)  # note this uses k=(observed value-1)
pval

# pval is less than alpha, so reject H0.

```

### Exampele 2: fair coin

```{r hypothesis_testing, cache = T, echo = T, eval = T, warnings = F, messages = F}

data <- c(1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
          1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1)

# Again, this is a binomial distribution for the observed number of heads, but this time the test is 2-tailed

# Write down the hypotheses
# H0: p = 1/2
# H1: p != 1/2

# choose a significance level
# alpha = 0.05
     

# find the number of heads
h <- sum(data)
h

# check number of trials
n <- length(data)
n

# now use H0 to find the p-value of the observed number of heads
ex <- 50 * 0.5 # the expected value
ex

x1 <- 20  # the lower tail
p1 <- pbinom(x1,50,0.5)  
pval <- 2 * p1 # double the p-value for a two-tailed test
print(pval)

# pval is greater than alpha, so we accept H0: there is no evidence that the coin is biased, at the 5% level.

```

## 3.2 Difference between two means: two sample t-test

We use the t test to assess whether two samples taken from normal distributions have significantly different means. The test statistic follows a Student's t-distribution, provided that the variances of the two groups are equal. Other variants of the t-test are applicable under different conditions.

```{r t_test, cache = T, echo = T, eval = T, warnings = F, messages = F}

data_nonsmoking <- c(3.99, 3.79, 3.60, 3.73, 3.21, 3.60, 4.08, 3.61, 3.83, 3.31, 4.13, 3.26, 3.54)
data_heavysmoking <- c(3.18, 2.84, 2.90, 3.27, 3.85, 3.52, 3.23, 2.76, 3.60, 3.75, 3.59, 3.63, 2.38, 2.34, 2.44)

# Write down the hypotheses
# H0: there is no difference in mean birth weight between groups: d == 0
# H1: there is a difference, d != 0

# choose a significance level
# alpha = 0.05
     
n_ns <- length(data_nonsmoking)
n_hs <- length(data_heavysmoking)

mean_ns <- mean(data_nonsmoking)
mean_hs <- mean(data_heavysmoking)

s_ns <- sd(data_nonsmoking)
s_hs <- sd(data_heavysmoking)

paste("non-smoking: n =", n_ns, ", mean =", mean_ns, ", SD =", s_ns)
paste("heavy smoking: n =", n_hs, ", mean =", mean_hs, ", SD =", s_hs)

# difference between the two sample means:
d_obs <- mean_ns - mean_hs
d_obs

# the pooled standard deviation
sp <- sqrt(((n_ns - 1)*s_ns^2 + (n_hs - 1)*s_hs^2)/(n_ns + n_hs - 2))
sp

# the test statistic
t_obs <- d_obs/(sp * sqrt(1/n_ns + 1/n_hs))
t_obs
     
# degrees of freedom is given by n1 + n2 - 2
df <- n_ns + n_hs - 2
df

# find the critical value
t95 <- qt(1-0.05/2,df) # critical value for 95% of probability mass
t95

# t_obs lies outside the 95% confidence interval [-t95,t95], so we reject H0

```