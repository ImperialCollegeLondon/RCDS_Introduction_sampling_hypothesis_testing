---
title: "Introduction to random sampling and hypothesis testing"
author: "Jesus Urtasun; jurtasun@ic.ac.uk"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  rmdformats::robobook:
    self_contained: true
    code_folding: hide
    thumbnails: false
    lightbox: true
    gallery: false
    highlight: tango
    number_sections: true
editor_options: 
  chunk_output_type: console
---

# Chapter 1

* Chapter1: random events and probability theory
* Import libraries and set working directory

```{r import, cache = T, echo = T, eval = T, warnings = F, messages = F}

# Import libraries
library(statip)
library(Rlab)

# Set working directory
workdir <- "/Users/jurtasun/Desktop/Courses/ICL/RCDS_Introduction_sampling_hypothesis_testing"
setwd(workdir)

# Function for plotting
set_plot_dimensions <- function(width_choice, height_choice) {
    options(repr.plot.width = width_choice, repr.plot.height = height_choice)
}

```

## 1.1 Random events and probability

In this course we will deal with random, or "stochastic" - rather that "deterministic" - processes. Without enteirng too much detail, we will describe a random event, plain and simple, as something whose output we don't know. In such context, we will define probability as a number between 0 and 1 that we use to quantify certainty, or surprise, about the output of a measurement.

There are many ways to give a proper definition of probability, which will be described in further chapter. The so called frequentist vs bayesian definitions probability. We will also comment on the meaning of the word susprise and its relation to significance when discussing hypothesis testing.

### Probability disteibutions

From now on, the terms random variable, probability distribution, and expectation value will be used to address random / stochastic processes. Probability distributions are mathematical objects we will use to describe how a series of random events are distributed, and from which we will actually compute expectation values, such as mean value, deviation or error, etc. A proper discussion on the so-called "momenta of a distribution" will be done further ahead. Throughout the course we will be running practical examples, hence the cell below shows a first chunk of python code to be executed.

### Bernoulli distribution

```{r bernoulli, cache = T, echo = T, eval = T, warnings = F, messages = F}

# Bernoulli distribution
p <- 1/6
x <- 0:1
pmf <- dbern(x,p)

# Plot probability mass function
set_plot_dimensions(4, 4)
plot(x, pmf, ylim = c(0, 1), xlab = "x", type = "h", col = "red", axes = FALSE)
points(x, pmf, col = "red")
axis(side = 1, at = c(0, 1))
axis(side = 2)

# Compute expected value
expectation <- sum(x * pmf)
expectation

# Compute variance
variance <- sum((x - expectation)^2 * pmf)
variance

```

## 1.2 Discrete distributions

By discrete we mean with a finite number of possible outputs. Processes like tossing coins, rolling dice, or counting the number of times a particular event happens / a specific measurement is made, are example of discrete distributions. The mathematical object we will use to describe such processes are referred to as mass distributions.

### Binomial distribution

```{r binomial, cache = T, echo = T, eval = T, warnings = F, messages = F}

# Binomial distribution
n <- 10
p <- 1/6
x <- 0:n
     
# Probability mass function
pmf <- dbinom(x,n,p)  # a binomial distribution with n=10, p=1/6

# Plot probability mass function
set_plot_dimensions(5, 4)
plot(x, pmf, ylim = c(0, 0.4), xlab = "x", type = "h", col = "red", axes = FALSE)
points(x, pmf, col = "red")
axis(side = 1, at = 0:10)
axis(side = 2)

# Cumulative distribution function
cdf <- pbinom(x,n,p)

# Plot cumulative distribution function
set_plot_dimensions(5, 4)
plot(x, cdf, ylim = c(0, 1), xlab = "x", type = "s", col = "blue", axes = FALSE)
axis(side = 1, at = x)
axis(side = 2)     

# Compute expected value
expectation <- sum(x * pmf)
expectation

# Compute variance
variance <- sum((x - expectation)^2 * pmf)
variance

# Probability of rolling one or more
1 - dbinom(0, n, p)

```

### Poisson distribution

```{r poisson, cache = T, echo = T, eval = T, warnings = F, messages = F}

 # Average of (e.g.) 4 meteorite impacts per year.
lambda = 4

# Probability mass function
x = 0:16
pmf <- dpois(x, lambda)

# Plot probability mass function
set_plot_dimensions(5, 4)
plot(x, pmf, ylim = c(0, 0.25), xlab = "x", type = "h", col = "red", axes = FALSE)
points(x, pmf, col = "red")
axis(side = 1, at = x)
axis(side = 2)

# Cumulative distribution function
cdf <- ppois(x,lambda)

# Plot cumulative distribution function
set_plot_dimensions(5, 4)
plot(x, cdf, ylim = c(0, 1), xlab = "x", type = "s", col = "blue", axes = FALSE)
axis(side = 1, at = x)
axis(side = 2)
     
# Compute expected value
expectation <- sum(x * pmf)  # approximates the sum for x->infinity
expectation

# Compute variance
variance <- sum((x - expectation)^2 * pmf) # approximates the sum for x->infinity
variance

# Probability of observing between 2 and 4 meteorite impacts
ppois(4,lambda) - ppois(1,lambda)

```

## 1.3 Continous distributions

We have discussed so far cases in which the number of outputs was discrete, meaning finite and countable. If we go back to the case of coins, we can only expect to have "heads", and "tails" as output, or in the case of rolling dice, every time we roll we expect either a 1, 2, 3, ..., 6. But what would happen if we are asked to compute the probability of measuring a specific temperature in a room, or a specific height of a given person?

Here we realize that most numerical variables we aim to describe will range in a continuous range, and that would really change the way we approach them. Indeed, the frequentist approach to define probability - i.e., as the number of times we get a specific result divided by the total number of outcomes, is allready ill-defined for these cases. The equivalent mathematical object we use for such cases will be adressed as density distribution.

### Gaussian distribution

```{r gaussian, cache = T, echo = T, eval = T, warnings = F, messages = F}

# Gaussian distribution - paper thickness in microns
mu <- 200
sigma <- 20

# Probability density function
wid <- 0.001
x <- seq(100, 300, wid)
pdf <- dnorm(x, mu, sigma) # a normal distribution
set_plot_dimensions(5, 4)
plot(x, pdf, xlab = "x", type = "l", col = "red")

# Cumulative distribution function
cdf <- pnorm(x, mu, sigma)
set_plot_dimensions(5, 4)
plot(x, cdf, xlab = "x", ylim = c(0, 1), type = "l", col = "blue")

# Compute expected value
expectation <- sum(x * pdf * wid)  # approximates the integral of [x * pdf(x)] dx
expectation

# Compute variance
variance <- sum( (x - expectation)^2 * pdf * wid) # approximating the integral of [(x - expectation)^2 * pdf] dx
variance

# Proportion of measurements are expected to be over 225
1 - pnorm(225,mu,sigma) # using the CDF

```

# SessionInfo

```{r, cache = T, echo = T, eval = T, warnings = F, messages = F, fig.height = 12, fig.width = 10}

sessionInfo()

```

